{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8caba290",
   "metadata": {},
   "source": [
    "-incarca datele din fisierele .pkl, extrage imaginile si etichetele\n",
    "\n",
    "-le transforma in vectori numerici si normalizează valorile pixelilor între 0 și 1, pregătind seturile de date pentru antrenarea modelului."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cee430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def load_extended_mnist(train_file=\"extended_mnist_train.pkl\",\n",
    "                        test_file=\"extended_mnist_test.pkl\"):\n",
    "    with open(train_file, \"rb\") as fp:\n",
    "        train = pickle.load(fp)\n",
    "    with open(test_file, \"rb\") as fp:\n",
    "        test = pickle.load(fp)\n",
    "\n",
    "    train_X, train_y = [], []\n",
    "    for img, lbl in train:\n",
    "        train_X.append(img.flatten())\n",
    "        train_y.append(lbl)\n",
    "\n",
    "    test_X = []\n",
    "    for img, _ in test:\n",
    "        test_X.append(img.flatten())\n",
    "\n",
    "    train_X = np.array(train_X, dtype=np.float32) / 255.0\n",
    "    train_y = np.array(train_y, dtype=np.int64)\n",
    "    test_X  = np.array(test_X,  dtype=np.float32) / 255.0\n",
    "    return train_X, train_y, test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b4a23",
   "metadata": {},
   "source": [
    "-transforma etichetele numerice (ex. 0–9) intr-o reprezentare binara (matrice one-hot), unde fiecare rand are valoarea 1 la pozitia clasei corespunzatoare si 0 in rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b01585cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels, num_classes=None):\n",
    "    if num_classes is None:\n",
    "        num_classes = int(labels.max()) + 1\n",
    "    Y = np.zeros((labels.shape[0], num_classes), dtype=np.float32)\n",
    "    Y[np.arange(labels.shape[0]), labels] = 1.0\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ece311",
   "metadata": {},
   "source": [
    "-creează parametrii modelului — matricea de greutăți W și vectorul de biase b — inițializate aleator (după regula Xavier) pentru a începe antrenarea rețelei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e61bdf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params(input_dim, output_dim):\n",
    "    limit = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "    W = np.random.default_rng(42).uniform(-limit, limit, (input_dim, output_dim)).astype(np.float32)\n",
    "    b = np.zeros((1, output_dim), dtype=np.float32)\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd27ab4",
   "metadata": {},
   "source": [
    "-transformă valorile de ieșire ale modelului în probabilități, asigurând că toate sunt pozitive și însumate la 1 pentru fiecare eșantion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa308f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    Z = Z - np.max(Z, axis=1, keepdims=True)\n",
    "    expZ = np.exp(Z)\n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c7a9f9",
   "metadata": {},
   "source": [
    "-calculează ieșirea modelului: combină intrările cu greutățile (X·W + b) și aplică funcția softmax pentru a obține probabilitățile pentru fiecare clasă."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6272b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, W, b):\n",
    "    Z = np.dot(X, W) + b\n",
    "    A = softmax(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1d11f9",
   "metadata": {},
   "source": [
    "-calculează eroarea medie (cross-entropy) dintre predicțiile modelului și etichetele reale; opțional adaugă un termen de regularizare L2 (weight_decay) pentru a reduce overfittingul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "667397f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(A, Y, W=None, weight_decay=0.0):\n",
    "    m = Y.shape[0]\n",
    "    eps = 1e-12\n",
    "    log_likelihood = -np.log(np.clip(A, eps, 1.0)[np.arange(m), Y.argmax(axis=1)])\n",
    "    ce = np.sum(log_likelihood) / m\n",
    "    if W is None or weight_decay <= 0:\n",
    "        return ce\n",
    "    return ce + 0.5 * weight_decay * np.sum(W * W)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a9df80",
   "metadata": {},
   "source": [
    "-calculează gradientele (derivatele) față de greutăți (dW) și biase (db), adică direcția în care trebuie ajustați parametrii pentru a reduce eroarea; include și efectul regularizării L2 dacă este activată."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ba8d174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(X, Y, A, W=None, weight_decay=0.0):\n",
    "    m = X.shape[0]\n",
    "    dZ = (A - Y) / m\n",
    "    dW = np.dot(X.T, dZ)\n",
    "    if weight_decay > 0 and W is not None:\n",
    "        dW = dW + weight_decay * W\n",
    "    db = np.sum(dZ, axis=0, keepdims=True)\n",
    "    return dW.astype(np.float32), db.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6db9bcc",
   "metadata": {},
   "source": [
    "-actualizează greutățile și biasele modelului folosind gradient descent; dacă este activat, aplică și momentum pentru a accelera convergența și a evita oscilațiile în procesul de antrenare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22735b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W, b, dW, db, learning_rate, momentum_state=None, momentum=0.0):\n",
    "    if momentum_state is None or momentum <= 0.0:\n",
    "        W -= learning_rate * dW\n",
    "        b -= learning_rate * db\n",
    "        return W, b, None\n",
    "    vW, vb = momentum_state\n",
    "    vW = momentum * vW - learning_rate * dW\n",
    "    vb = momentum * vb - learning_rate * db\n",
    "    W += vW\n",
    "    b += vb\n",
    "    return W, b, (vW, vb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b472b",
   "metadata": {},
   "source": [
    "-realizează antrenarea efectivă a modelului: inițializează parametrii, parcurge datele în mini-loturi (batch-uri), face forward propagation, calculează eroarea și gradientele, apoi actualizează parametrii la fiecare pas. Repetă procesul pentru mai multe epoci, ajustând treptat learning rate-ul și aplicând momentum sau regularizare dacă sunt setate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a548225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_X, train_Y, input_dim, output_dim,\n",
    "                epochs=100, learning_rate=0.01, batch_size=100,\n",
    "                momentum=0.9, weight_decay=3e-4, lr_decay=1.0,\n",
    "                verbose=True):\n",
    "    W, b = initialize_params(input_dim, output_dim)\n",
    "    momentum_state = (np.zeros_like(W), np.zeros_like(b)) if momentum > 0 else None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        perm = np.random.permutation(train_X.shape[0])\n",
    "        train_X = train_X[perm]\n",
    "        train_Y = train_Y[perm]\n",
    "\n",
    "        for i in range(0, train_X.shape[0], batch_size):\n",
    "            X_batch = train_X[i:i+batch_size]\n",
    "            Y_batch = train_Y[i:i+batch_size]\n",
    "\n",
    "            A = forward_propagation(X_batch, W, b)\n",
    "            loss = compute_loss(A, Y_batch, W, weight_decay)\n",
    "            dW, db = backward_propagation(X_batch, Y_batch, A, W, weight_decay)\n",
    "            W, b, momentum_state = update_params(W, b, dW, db, learning_rate,\n",
    "                                                 momentum_state, momentum)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}')\n",
    "        learning_rate *= lr_decay\n",
    "\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47acac66",
   "metadata": {},
   "source": [
    "-calculează ieșirile modelului pentru datele de intrare și returnează clasa cu probabilitatea cea mai mare pentru fiecare exemplu (predicția finală)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68903f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    A = forward_propagation(X, W, b)\n",
    "    return np.argmax(A, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809e847d",
   "metadata": {},
   "source": [
    "-această parte încarcă datele de antrenare și test, apoi împarte setul de antrenare în 90% pentru antrenare și 10% pentru validare, păstrând proporția claselor. În final, etichetele de antrenare sunt transformate în reprezentare one-hot, necesară pentru calculul corect al erorii în timpul antrenării"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08efd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y, test_X = load_extended_mnist(\n",
    "    train_file=\"fii-nn-2025-homework-2/extended_mnist_train.pkl\",\n",
    "    test_file=\"fii-nn-2025-homework-2/extended_mnist_test.pkl\"\n",
    ")\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "idx = np.arange(len(train_y))\n",
    "val_mask = np.zeros_like(idx, dtype=bool)\n",
    "for c in range(train_y.max()+1):\n",
    "    ids = idx[train_y == c]\n",
    "    rng.shuffle(ids)\n",
    "    k = max(1, int(0.10 * len(ids)))\n",
    "    val_mask[ids[:k]] = True\n",
    "X_tr, y_tr = train_X[~val_mask], train_y[~val_mask]\n",
    "X_val, y_val = train_X[val_mask], train_y[val_mask]\n",
    "\n",
    "num_classes = int(train_y.max()) + 1\n",
    "train_Y_oh = one_hot_encode(y_tr, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bec0411",
   "metadata": {},
   "source": [
    "-setează hiper-parametrii antrenării: dimensiunea intrării și ieșirii, numărul de epoci, rata de învățare, mărimea batch-ului, momentum, regularizarea L2 (weight_decay) și scăderea treptată a LR (lr_decay). Acestea controlează cum și cât de repede învață modelul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67ad9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_X.shape[1]\n",
    "output_dim = num_classes\n",
    "epochs = 40\n",
    "learning_rate = 0.25\n",
    "batch_size = 256\n",
    "momentum = 0.9\n",
    "weight_decay = 3e-4\n",
    "lr_decay = 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2fbaba",
   "metadata": {},
   "source": [
    "-această parte antrenează modelul folosind funcția train_model() cu hiperparametrii definiți, apoi face predicții pe seturile de antrenare și validare pentru a calcula și afișa acuratețea modelului — adică cât de bine a învățat să clasifice imaginile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be1d305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, Loss: 0.3353\n",
      "Epoch 2/40, Loss: 0.2693\n",
      "Epoch 3/40, Loss: 0.3449\n",
      "Epoch 4/40, Loss: 0.2991\n",
      "Epoch 5/40, Loss: 0.3219\n",
      "Epoch 6/40, Loss: 0.3033\n",
      "Epoch 7/40, Loss: 0.3888\n",
      "Epoch 8/40, Loss: 0.2997\n",
      "Epoch 9/40, Loss: 0.2523\n",
      "Epoch 10/40, Loss: 0.3647\n",
      "Epoch 11/40, Loss: 0.3716\n",
      "Epoch 12/40, Loss: 0.3612\n",
      "Epoch 13/40, Loss: 0.3586\n",
      "Epoch 14/40, Loss: 0.3074\n",
      "Epoch 15/40, Loss: 0.3501\n",
      "Epoch 16/40, Loss: 0.4127\n",
      "Epoch 17/40, Loss: 0.2928\n",
      "Epoch 18/40, Loss: 0.3418\n",
      "Epoch 19/40, Loss: 0.3774\n",
      "Epoch 20/40, Loss: 0.2686\n",
      "Epoch 21/40, Loss: 0.2619\n",
      "Epoch 22/40, Loss: 0.2619\n",
      "Epoch 23/40, Loss: 0.3864\n",
      "Epoch 24/40, Loss: 0.2920\n",
      "Epoch 25/40, Loss: 0.2418\n",
      "Epoch 26/40, Loss: 0.2415\n",
      "Epoch 27/40, Loss: 0.4098\n",
      "Epoch 28/40, Loss: 0.3063\n",
      "Epoch 29/40, Loss: 0.3237\n",
      "Epoch 30/40, Loss: 0.3258\n",
      "Epoch 31/40, Loss: 0.2635\n",
      "Epoch 32/40, Loss: 0.3213\n",
      "Epoch 33/40, Loss: 0.2584\n",
      "Epoch 34/40, Loss: 0.2752\n",
      "Epoch 35/40, Loss: 0.2980\n",
      "Epoch 36/40, Loss: 0.2686\n",
      "Epoch 37/40, Loss: 0.2830\n",
      "Epoch 38/40, Loss: 0.2979\n",
      "Epoch 39/40, Loss: 0.2766\n",
      "Epoch 40/40, Loss: 0.2690\n",
      "Training Accuracy: 92.85%\n",
      "Validation Accuracy: 92.06%\n"
     ]
    }
   ],
   "source": [
    "W, b = train_model(X_tr, train_Y_oh, input_dim, output_dim,\n",
    "                   epochs=epochs, learning_rate=learning_rate, batch_size=batch_size,\n",
    "                   momentum=momentum, weight_decay=weight_decay, lr_decay=lr_decay,\n",
    "                   verbose=True)\n",
    "\n",
    "train_pred = predict(X_tr, W, b)\n",
    "val_pred = predict(X_val, W, b)\n",
    "print(f'Training Accuracy: {accuracy_score(y_tr, train_pred) * 100:.2f}%')\n",
    "print(f'Validation Accuracy: {accuracy_score(y_val, val_pred) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd3840",
   "metadata": {},
   "source": [
    "-această parte re-antrenează modelul pe întreg setul de date pentru performanță maximă, apoi generează predicțiile finale pentru imaginile de test. Rezultatele sunt salvate într-un fișier submission.csv, în formatul cerut (ID, target), pentru a fi trimise spre evaluare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1693c161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved submission.csv\n"
     ]
    }
   ],
   "source": [
    "Y_full_oh = one_hot_encode(train_y, num_classes)\n",
    "W_final, B_final = train_model(train_X, Y_full_oh, input_dim, output_dim,\n",
    "                               epochs=28, learning_rate=0.20, batch_size=batch_size,\n",
    "                               momentum=momentum, weight_decay=weight_decay, lr_decay=lr_decay,\n",
    "                               verbose=False)\n",
    "\n",
    "test_predictions = predict(test_X, W_final, B_final)\n",
    "\n",
    "predictions_csv = {\"ID\": np.arange(len(test_predictions), dtype=int),\n",
    "                   \"target\": test_predictions.astype(int)}\n",
    "pd.DataFrame(predictions_csv).to_csv(\"submission.csv\", index=False)\n",
    "print(\"Saved submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
